{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wfdb in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (2.2.1)\n",
      "Requirement already satisfied: requests>=2.10.0 in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from wfdb) (2.22.0)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from wfdb) (1.4.1)\n",
      "Requirement already satisfied: sklearn>=0.0 in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from wfdb) (0.0)\n",
      "Requirement already satisfied: nose>=1.3.7 in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from wfdb) (1.3.7)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from wfdb) (1.17.2)\n",
      "Requirement already satisfied: matplotlib>=1.5.1 in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from wfdb) (3.1.1)\n",
      "Requirement already satisfied: pandas>=0.19.1 in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from wfdb) (0.25.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.10.0->wfdb) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.10.0->wfdb) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.10.0->wfdb) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.10.0->wfdb) (2.8)\n",
      "Requirement already satisfied: scikit-learn in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from sklearn>=0.0->wfdb) (0.21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from matplotlib>=1.5.1->wfdb) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from matplotlib>=1.5.1->wfdb) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from matplotlib>=1.5.1->wfdb) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from matplotlib>=1.5.1->wfdb) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from pandas>=0.19.1->wfdb) (2019.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from scikit-learn->sklearn>=0.0->wfdb) (0.13.2)\n",
      "Requirement already satisfied: six in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from cycler>=0.10->matplotlib>=1.5.1->wfdb) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib>=1.5.1->wfdb) (41.4.0)\n",
      "Requirement already satisfied: biosppy in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (0.6.1)\n",
      "Requirement already satisfied: bidict in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from biosppy) (0.19.0)\n",
      "Requirement already satisfied: scipy in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from biosppy) (1.4.1)\n",
      "Requirement already satisfied: h5py in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from biosppy) (2.9.0)\n",
      "Requirement already satisfied: shortuuid in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from biosppy) (0.5.0)\n",
      "Requirement already satisfied: six in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from biosppy) (1.12.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from biosppy) (0.21.3)\n",
      "Requirement already satisfied: matplotlib in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from biosppy) (3.1.1)\n",
      "Requirement already satisfied: numpy in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from biosppy) (1.17.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from scikit-learn->biosppy) (0.13.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from matplotlib->biosppy) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from matplotlib->biosppy) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from matplotlib->biosppy) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from matplotlib->biosppy) (2.8.0)\n",
      "Requirement already satisfied: setuptools in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->biosppy) (41.4.0)\n",
      "Requirement already satisfied: opencv-python in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (4.2.0.32)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from opencv-python) (1.17.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install wfdb\n",
    "!pip install biosppy\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "from glob import glob\n",
    "import re\n",
    "import wfdb\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import biosppy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.callbacks import History\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPool2D, ELU, Dropout, Dense, Flatten\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "TRAIN_PATH = 'train-imgs/'\n",
    "LABEL_TRAIN_PATH = 'labels-train.txt'\n",
    "\n",
    "TEST_PATH = 'test-imgs/'\n",
    "LABEL_TEST_PATH = 'labels-test.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_record(record, index):\n",
    "    fig, ax = plt.subplots(nrows=2, figsize=(18,6))\n",
    "    I = record[0][:, 0]\n",
    "    II = record[0][:, 1]\n",
    "\n",
    "    ax[0].plot(I)\n",
    "    ax[1].plot(II)\n",
    "    ax[0].set_ylabel('Lead II ' + index, color='B', fontweight='bold', fontsize='large')\n",
    "    ax[1].set_xlabel('Datapoints' + index, color='B', fontweight='bold', fontsize='large')\n",
    "    ax[1].set_ylabel('Lead II' + index, color='B', fontweight='bold', fontsize='large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in get_file_indexes():\n",
    "#     plot_record(wfdb.rdsamp('mitdb/' + file, sampto=1000), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_idx = {\n",
    "    'apc': 0,\n",
    "    'nor': 1, \n",
    "    'lbb': 2, \n",
    "    'pab': 3, \n",
    "    'pvc': 4, \n",
    "    'rbb': 5,  \n",
    "    'veb': 6, \n",
    "    'vfw': 7\n",
    "}\n",
    "symbol_to_label = {\n",
    "    'N':'nor', \n",
    "    'L':'lbb', \n",
    "    'R':'rbb', \n",
    "    'A':'apc', \n",
    "    'V':'pvc', \n",
    "    '/':'pab', \n",
    "    'E':'veb', \n",
    "    '!':'vfw'\n",
    "}\n",
    "idx_to_class = dict(zip(class_to_idx.values(), class_to_idx.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_paths = glob(TEST_PATH + '*.png')\n",
    "\n",
    "cropped_test_paths = {}\n",
    "begining = test_paths[0].rfind('/') + 1\n",
    "nums = []\n",
    "for path in test_paths:\n",
    "    end = path.rfind('.')\n",
    "    \n",
    "    number = path[begining:end]\n",
    "    nums.append(number)\n",
    "    record_n = int(number[:number.find('_')])\n",
    "    sig_n = int(number[number.find('_') + 1:])\n",
    "    \n",
    "    if cropped_test_paths.get(record_n) is None:\n",
    "        cropped_test_paths[record_n] = 0\n",
    "    cropped_test_paths[record_n] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1863, 1: 2567, 2: 1873, 3: 2278, 4: 1580, 5: 1853, 6: 2618, 7: 1948}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(sorted(cropped_test_paths.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16973\n",
      "16580\n",
      "16973\n"
     ]
    }
   ],
   "source": [
    "id_labels_test = {}\n",
    "count = 0\n",
    "lines = 0\n",
    "with open(LABEL_TEST_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        lines += 1\n",
    "        n, l = line.split()\n",
    "        if n in nums:\n",
    "            count += 1\n",
    "        id_labels_test[n] = int(l)\n",
    "\n",
    "len(id_labels_test)\n",
    "print(count)\n",
    "print(len(nums))\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(list(id_labels_test.values()))\n",
    "NUM_CLASSES = len(np.unique(labels))\n",
    "\n",
    "indices = np.arange(len(labels))\n",
    "np.random.seed(123)\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cropping_images(image):\n",
    "    #Left Top Crop\n",
    "    left_top = cv2.resize(image[:96, :96], (128, 128))\n",
    "\n",
    "    #Center Top Crop\n",
    "    center_top = cv2.resize(image[:96, 16:112], (128, 128))\n",
    "\n",
    "    #Right Top Crop\n",
    "    right_top = cv2.resize(image[:96, 32:], (128, 128))\n",
    "\n",
    "    #Left Center Crop\n",
    "    left_center = cv2.resize(image[16:112, :96], (128, 128))\n",
    "\n",
    "    #Center Center Crop\n",
    "    center_center = cv2.resize(image[16:112, 16:112], (128, 128))\n",
    "\n",
    "    #Right Center Crop    \n",
    "    right_center = cv2.resize(image[16:112, 32:], (128, 128))\n",
    "\n",
    "    #Left Bottom Crop\n",
    "    left_bottom = cv2.resize(image[32:, :96], (128, 128))\n",
    "\n",
    "    #Center Bottom Crop\n",
    "    center_bottom = cv2.resize(image[32:, 16:112], (128, 128))\n",
    "\n",
    "    #Right Bottom Crop    \n",
    "    right_bottom = cv2.resize(image[32:, 32:], (128, 128))\n",
    "\n",
    "    return np.array([left_top, center_top, right_top,\n",
    "            left_center, center_center, right_center,\n",
    "            left_bottom, center_bottom, right_bottom])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator(ind, augment=False):\n",
    "    image = cv2.imread(test_paths[ind], cv2.IMREAD_GRAYSCALE)\n",
    "    number_path = test_paths[ind][test_paths[ind].rfind('/') + 1 : test_paths[ind].rfind('.')]\n",
    "   \n",
    "    label = id_labels_test[number_path]\n",
    "    \n",
    "    if augment and label != class_to_idx['nor']:\n",
    "        cropped_images = get_cropping_images(image)\n",
    "        images = np.vstack((np.expand_dims(image, axis=0), cropped_images)) \n",
    "        yield images, [label] * len(images)\n",
    "    else:\n",
    "        yield np.expand_dims(image, 0), [label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_batch_generator(batch_size, augment=False, debug=False):\n",
    "    global batch_i\n",
    "    \n",
    "    generators = np.array([get_generator(ind, augment) for ind in range(len(test_paths))])\n",
    "    while True:\n",
    "        batch_indices = indices[(batch_i - 1) * batch_size : batch_i * batch_size]\n",
    "        batch_i += 1\n",
    "        yield [gen.__next__() for gen in generators[batch_indices]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_and_labels_generator(batch_size, augment=False):\n",
    "    for batch in raw_batch_generator(batch_size, augment):\n",
    "        batch_images = []\n",
    "        batch_labels = []\n",
    "        for e in batch:\n",
    "            batch_images.append(e[0])\n",
    "            batch_labels.extend(e[1])\n",
    "        batch_images = np.stack(batch_images, axis=0) if not augment else np.vstack(batch_images)\n",
    "        yield batch_images, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iterator_test(batch_size, augment=False):      \n",
    "    for batch in images_and_labels_generator(batch_size, augment):\n",
    "        batch_images = batch[0]\n",
    "        batch_images = np.expand_dims(batch_images, -1)\n",
    "        batch_labels = keras.utils.to_categorical(batch[1], NUM_CLASSES)\n",
    "        yield batch_images, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140, 128, 128, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "batch_i = 1\n",
    "train_iterator_test(32, augment=True).__next__()[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get images paths and get images IDs from the paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = glob(TRAIN_PATH + '*.png')\n",
    "\n",
    "cropped_train_paths = {}\n",
    "begining = train_paths[0].rfind('/') + 1\n",
    "\n",
    "for path in train_paths:\n",
    "    end = path.rfind('.')\n",
    "    \n",
    "    number = path[begining:end]\n",
    "    record_n = int(number[:number.find('_')])\n",
    "    sig_n = int(number[number.find('_') + 1:])\n",
    "    \n",
    "    if cropped_train_paths.get(record_n) is None:\n",
    "        cropped_train_paths[record_n] = 0\n",
    "    cropped_train_paths[record_n] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91088"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_labels_train = {}\n",
    "with open(LABEL_TRAIN_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        n, l = line.split()\n",
    "        id_labels_train[n] = int(l)\n",
    "\n",
    "len(id_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(list(id_labels_train.values()))\n",
    "NUM_CLASSES = len(np.unique(labels))\n",
    "\n",
    "indices = np.arange(len(labels))\n",
    "np.random.seed(123)\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tha dataset is imbalanced, so we'll need to artificially augment smaller classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.unique(list(id_labels_train.values()), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "objects = a[0]\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = a[1]\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.barh(y_pos, performance, align='center')\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.unique(list(id_labels_train.values()), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "height = r[1]\n",
    "bars = r[0]\n",
    "y_pos = np.arange(len(bars))\n",
    "\n",
    "\n",
    "plt.bar(y_pos, height, color=['red'])\n",
    "plt.xticks(y_pos, bars)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This method augments smaller classes ten times cropping and shifting images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cropping_images(image):\n",
    "    #Left Top Crop\n",
    "    left_top = cv2.resize(image[:96, :96], (128, 128))\n",
    "\n",
    "    #Center Top Crop\n",
    "    center_top = cv2.resize(image[:96, 16:112], (128, 128))\n",
    "\n",
    "    #Right Top Crop\n",
    "    right_top = cv2.resize(image[:96, 32:], (128, 128))\n",
    "\n",
    "    #Left Center Crop\n",
    "    left_center = cv2.resize(image[16:112, :96], (128, 128))\n",
    "\n",
    "    #Center Center Crop\n",
    "    center_center = cv2.resize(image[16:112, 16:112], (128, 128))\n",
    "\n",
    "    #Right Center Crop    \n",
    "    right_center = cv2.resize(image[16:112, 32:], (128, 128))\n",
    "\n",
    "    #Left Bottom Crop\n",
    "    left_bottom = cv2.resize(image[32:, :96], (128, 128))\n",
    "\n",
    "    #Center Bottom Crop\n",
    "    center_bottom = cv2.resize(image[32:, 16:112], (128, 128))\n",
    "\n",
    "    #Right Bottom Crop    \n",
    "    right_bottom = cv2.resize(image[32:, 32:], (128, 128))\n",
    "\n",
    "    return np.array([left_top, center_top, right_top,\n",
    "            left_center, center_center, right_center,\n",
    "            left_bottom, center_bottom, right_bottom])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be consistent with the 1D test/train split, we'll define a list of test indexes corresponding to their records\n",
    "\n",
    "1D list: ```['101', '105', '114', '118', '124', '201', '210', '217']```  \n",
    "2D list: ```[1, 5, 13, 17, 22, 24, 31, 36]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator(ind, augment=False):\n",
    "    image = cv2.imread(train_paths[ind], cv2.IMREAD_GRAYSCALE)\n",
    "    number_path = train_paths[ind][train_paths[ind].rfind('/') + 1 : train_paths[ind].rfind('.')]\n",
    "   \n",
    "    label = id_labels_train[number_path]\n",
    "    \n",
    "    if augment and label != class_to_idx['nor']:\n",
    "        cropped_images = get_cropping_images(image)\n",
    "        images = np.vstack((np.expand_dims(image, axis=0), cropped_images)) \n",
    "        yield images, [label] * len(images)\n",
    "    else:\n",
    "        yield np.expand_dims(image, 0), [label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_batch_generator(batch_size, augment=False, debug=False):\n",
    "    global batch_i\n",
    "    \n",
    "    generators = np.array([get_generator(ind, augment) for ind in range(len(train_paths))])\n",
    "    while True:\n",
    "        batch_indices = indices[(batch_i - 1) * batch_size : batch_i * batch_size]\n",
    "        batch_i += 1\n",
    "        yield [gen.__next__() for gen in generators[batch_indices]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_and_labels_generator(batch_size, augment=False):\n",
    "    for batch in raw_batch_generator(batch_size, augment):\n",
    "        batch_images = []\n",
    "        batch_labels = []\n",
    "        for e in batch:\n",
    "            batch_images.append(e[0])\n",
    "            batch_labels.extend(e[1])\n",
    "        batch_images = np.stack(batch_images, axis=0) if not augment else np.vstack(batch_images)\n",
    "        yield batch_images, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iterator(batch_size, augment=False):      \n",
    "    for batch in images_and_labels_generator(batch_size, augment):\n",
    "        batch_images = batch[0]\n",
    "        batch_images = np.expand_dims(batch_images, -1)\n",
    "        batch_labels = keras.utils.to_categorical(batch[1], NUM_CLASSES)\n",
    "        yield batch_images, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Keras in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (2.3.1)\n",
      "Requirement already satisfied: h5py in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from Keras) (2.9.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from Keras) (1.12.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from Keras) (1.0.8)\n",
      "Requirement already satisfied: scipy>=0.14 in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from Keras) (1.4.1)\n",
      "Requirement already satisfied: pyyaml in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from Keras) (5.1.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from Keras) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /Users/miki/opt/anaconda3/lib/python3.7/site-packages (from Keras) (1.17.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91088"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91088"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149, 128, 128, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "batch_i = 1\n",
    "train_iterator(32, augment=True).__next__()[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSaveCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, file_name):\n",
    "        super(ModelSaveCallback, self).__init__()\n",
    "        self.file_name = file_name\n",
    "        self.f1s = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):     \n",
    "        model_filename = self.file_name.format(epoch % 3)\n",
    "        save_model(self.model, model_filename)\n",
    "        print(\"Model saved in {}\".format(model_filename))\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom F1 score mectric for measuring training quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    f1_val = 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "            \n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define parameters for CNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "STEPS_PER_EPOCH = 10\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, (3, 3),strides=(1,1), input_shape=(IMAGE_SIZE, IMAGE_SIZE, 1), kernel_initializer='glorot_uniform'))\n",
    "model.add(ELU())\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(64, (3, 3),strides=(1, 1),kernel_initializer='glorot_uniform'))\n",
    "model.add(ELU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), strides = (1, 1),kernel_initializer='glorot_uniform'))\n",
    "model.add(ELU())\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(128, (3, 3),strides=(1, 1),kernel_initializer='glorot_uniform'))\n",
    "model.add(ELU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), strides=(1, 1),kernel_initializer='glorot_uniform'))\n",
    "model.add(ELU())\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(256, (3, 3),strides=(1, 1),kernel_initializer='glorot_uniform'))\n",
    "model.add(ELU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2048))\n",
    "model.add(ELU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "  loss='categorical_crossentropy',\n",
    "  optimizer='adam',  # gradient clipping just in case\n",
    "  metrics=['accuracy'] \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count class weights to cope with imbalanced dataset during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_for_weights = labels\n",
    "for ind in [0, 2, 3, 4, 5, 6, 7]:\n",
    "    new_labels = np.full(sum(labels == ind) * 9, ind)\n",
    "    labels_for_weights = np.append(labels_for_weights, new_labels)\n",
    "print(\"Weights: \" + str(len(labels_for_weights)))\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(labels_for_weights), labels_for_weights)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FNAME = 'model_v3_{}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_from_n_epoch(initial_epoch, load_fname):\n",
    "    global batch_i\n",
    "    global hist\n",
    "    \n",
    "    batch_i = initial_epoch * STEPS_PER_EPOCH + 1\n",
    "    hist = History()\n",
    "    model = load_model(load_fname, custom_objects={'f1':f1})\n",
    "\n",
    "    model.fit_generator(\n",
    "        train_iterator(BATCH_SIZE, augment=True), \n",
    "        steps_per_epoch=STEPS_PER_EPOCH,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[ModelSaveCallback(MODEL_FNAME), hist],\n",
    "        verbose=1,  \n",
    "        class_weight=class_weights,\n",
    "        initial_epoch=initial_epoch\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_i = 1\n",
    "hist = History()\n",
    "\n",
    "H = model.fit(\n",
    "    train_iterator(BATCH_SIZE, True), \n",
    "    epochs=3,\n",
    "    callbacks=[ModelSaveCallback(MODEL_FNAME), hist],\n",
    "    verbose=1,  \n",
    "    class_weight=class_weights,\n",
    "    initial_epoch=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(H.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(H.history[\"val_acc\"], label=\"val_acc\")\n",
    "\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('f1_scores.txt', 'a') as fout:\n",
    "    for val in hist.history['f1']:\n",
    "        fout.write(str(val) + '\\n')\n",
    "        \n",
    "with open('cat_accuracies.txt', 'a') as fout:\n",
    "    for val in hist.history['categorical_accuracy']:\n",
    "        fout.write(str(val) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_iterator(paths_test):\n",
    "    for i in range(len(paths_test)):\n",
    "        image = cv2.imread(paths_test[i], cv2.IMREAD_GRAYSCALE)\n",
    "        image = np.expand_dims(image, -1)\n",
    "        yield np.expand_dims(image, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test = []\n",
    "for path in all_paths:\n",
    "    img_n = path[path.rfind('/') + 1 : path.rfind('.')]\n",
    "    labels_test.append(n_l[img_n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict_generator(predict_iterator(all_paths), steps=len(all_paths), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pred = np.argmax(preds, axis=1)\n",
    "labels_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate confusion matrix and f1-score for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mtx = confusion_matrix(labels_test, labels_pred, labels=np.arange(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "for i in range(NUM_CLASSES):\n",
    "    x.append(idx_to_class[i].upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 5))\n",
    "ax = sns.heatmap(conf_mtx_1, annot=True, fmt='d', cmap=sns.cm.rocket_r, \n",
    "                 xticklabels=x, yticklabels=x, robust=True, \n",
    "                 linewidths=0.1, linecolor='grey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = f1_score(labels_pred, labels_test, average=None)\n",
    "f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(f1_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
